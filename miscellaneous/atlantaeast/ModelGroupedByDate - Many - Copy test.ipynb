{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "ERZNMl3_bllo",
    "outputId": "44a3309f-fff1-49aa-9fdc-0ce52a81a674"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_rate_day</th>\n",
       "      <th>max_rate_day</th>\n",
       "      <th>min_rate_day</th>\n",
       "      <th>mean_rate_mile</th>\n",
       "      <th>max_rate_mile</th>\n",
       "      <th>min_rate_mile</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>...</th>\n",
       "      <th>2022</th>\n",
       "      <th>SumOfFLEET All</th>\n",
       "      <th>Utilization Rate All</th>\n",
       "      <th>temp</th>\n",
       "      <th>precip</th>\n",
       "      <th>snowdepth</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>DayOfWeek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50.835000</td>\n",
       "      <td>69.99</td>\n",
       "      <td>31.68</td>\n",
       "      <td>0.435600</td>\n",
       "      <td>0.8712</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75</td>\n",
       "      <td>0.613333</td>\n",
       "      <td>42.7</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48.480000</td>\n",
       "      <td>69.99</td>\n",
       "      <td>32.46</td>\n",
       "      <td>0.620400</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73</td>\n",
       "      <td>0.671233</td>\n",
       "      <td>45.7</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42.082857</td>\n",
       "      <td>69.99</td>\n",
       "      <td>33.44</td>\n",
       "      <td>0.818743</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>51.2</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.390000</td>\n",
       "      <td>69.99</td>\n",
       "      <td>35.19</td>\n",
       "      <td>0.913733</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>0.8712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72</td>\n",
       "      <td>0.597222</td>\n",
       "      <td>57.7</td>\n",
       "      <td>1.319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.990000</td>\n",
       "      <td>29.99</td>\n",
       "      <td>29.99</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72</td>\n",
       "      <td>0.569444</td>\n",
       "      <td>40.8</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_rate_day  max_rate_day  min_rate_day  mean_rate_mile  max_rate_mile  \\\n",
       "0      50.835000         69.99         31.68        0.435600         0.8712   \n",
       "1      48.480000         69.99         32.46        0.620400         0.9900   \n",
       "2      42.082857         69.99         33.44        0.818743         0.9900   \n",
       "3      47.390000         69.99         35.19        0.913733         0.9900   \n",
       "4      29.990000         29.99         29.99        0.250000         0.2500   \n",
       "\n",
       "   min_rate_mile  2015  2016  2017  2018  ...  2022  SumOfFLEET All  \\\n",
       "0         0.0000   NaN   NaN   NaN   NaN  ...   NaN              75   \n",
       "1         0.0000   NaN   NaN   NaN   NaN  ...   NaN              73   \n",
       "2         0.0000   NaN   NaN   NaN   NaN  ...   NaN              70   \n",
       "3         0.8712   NaN   NaN   NaN   NaN  ...   NaN              72   \n",
       "4         0.2500   NaN   NaN   NaN   NaN  ...   NaN              72   \n",
       "\n",
       "   Utilization Rate All  temp  precip  snowdepth  Year  Month  Day  DayOfWeek  \n",
       "0              0.613333  42.7   0.060        0.0  2015      1    1          3  \n",
       "1              0.671233  45.7   0.851        0.0  2015      1    2          4  \n",
       "2              0.642857  51.2   0.221        0.0  2015      1    3          5  \n",
       "3              0.597222  57.7   1.319        0.0  2015      1    4          6  \n",
       "4              0.569444  40.8   0.000        0.0  2015      1    5          0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "#Reload the dataset\n",
    "data = pd.read_excel('../atlanta_east/Daily_Data_AtlantaEastNA.xlsx')\n",
    "\n",
    "# Convert the 'Date' column to datetime\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "data['Year'] = data['Date'].dt.year\n",
    "data['Month'] = data['Date'].dt.month\n",
    "data['Day'] = data['Date'].dt.day\n",
    "data['DayOfWeek'] = data['Date'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "\n",
    "# Dropping the original 'Date' column\n",
    "data = data.drop(columns=['DATE OUT', 'Date', 'Unnamed: 0'])\n",
    "\n",
    "# Checking the updated dataframe\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oH4t9d9hf92f"
   },
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modelling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Tree Visualisation\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AjoysQSvcv5Z"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "missing_values = data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GVOChNasccfl"
   },
   "outputs": [],
   "source": [
    "#%% train and test sets\n",
    "data=data.dropna()\n",
    "split = int(.8*len(data))\n",
    "\n",
    "X_train = data.loc[:split,:].drop(['Utilization Rate All'],axis=1)\n",
    "y_train = data.loc[:split,'Utilization Rate All']\n",
    "\n",
    "X_test = data.loc[split:,:].drop(['Utilization Rate All'],axis=1)\n",
    "y_test = data.loc[split:,'Utilization Rate All']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2HGhjLwsfzAK",
    "outputId": "41f05460-27c9-41c5-a238-6b9ec6b7af9a"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 22)) while a minimum of 1 is required by LinearRegression.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m lr \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Predict on the test set\u001b[39;00m\n\u001b[0;32m     12\u001b[0m y_pred_lr \u001b[38;5;241m=\u001b[39m lr\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:678\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    674\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[0;32m    676\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 678\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    680\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m has_sw \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_sw:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    620\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 622\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1146\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1143\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1144\u001b[0m     )\n\u001b[1;32m-> 1146\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1162\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1164\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:967\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    965\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m--> 967\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    968\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    969\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    970\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m    971\u001b[0m         )\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    974\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 22)) while a minimum of 1 is required by LinearRegression."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error for the Linear Regression model\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "print(f\"Linear Regression MSE: {mse_lr}\")\n",
    "\n",
    "r_squared = r2_score(y_test, y_pred_lr)\n",
    "print(\"R-squared:\", r_squared)\n",
    "\n",
    "# Sorting the coefficients in descending order\n",
    "sorted_coeffs = sorted(zip(X_train.columns, lr.coef_), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the intercept\n",
    "print(\"Intercept:\", lr.intercept_)\n",
    "\n",
    "# Print the sorted coefficients for each feature\n",
    "for feature, coef in sorted_coeffs:\n",
    "    print(f\"{feature}: {coef}\")\n",
    "\n",
    "\n",
    "print('________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "i479MiBSf5P-",
    "outputId": "626095e4-1466-4927-d4ee-0c6419a6e3cf"
   },
   "outputs": [],
   "source": [
    "# Linear Regression Predictions vs Actual\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test, y_pred_lr, color='red', alpha=0.5, label='Predictions')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='blue', label='y=x line')\n",
    "plt.title('Linear Regression: Predictions vs Actual')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "plt.axis('square')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZE9URMVWgBfT",
    "outputId": "601d260d-0ee2-4ee2-e5f6-55ea83dfc7a1"
   },
   "outputs": [],
   "source": [
    "features = data.drop(['Utilization Rate All'], axis=1)\n",
    "labels = data['Utilization Rate All']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.20, random_state = 12)\n",
    "\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)\n",
    "\n",
    "feature_list = list(features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q7Ix__-egTf8",
    "outputId": "56b371e8-4f61-4ee5-e970-8fd7aa154193"
   },
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 12)\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels);\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - test_labels)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / test_labels)\n",
    "mse_rf = mean_squared_error(test_labels, predictions)\n",
    "print(f\"RandomForest MSE: {mse_rf}\")\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "score = r2_score(test_labels, predictions)\n",
    "print(\"The accuracy of our model is {}%\".format(round(score, 2) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "zGDGLQsGgY8w",
    "outputId": "b032c0ec-aebc-4722-aa85-63a17097e641"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Random Forest Predictions vs Actual\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(test_labels, predictions, color='red', alpha=0.5, label='Predictions')\n",
    "plt.plot([min(test_labels), max(test_labels)], [min(test_labels), max(test_labels)], color='blue', label='y=x line')\n",
    "plt.title('RandomForest: Predictions vs Actual')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "plt.axis('square')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "cP7tDLwPghZJ",
    "outputId": "10310068-77b0-43da-b245-db0086caa719"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "x = data.drop(['Utilization Rate All', 'Year'], axis=1)\n",
    "y = data['Utilization Rate All']\n",
    "\n",
    "xtrain, xtest, ytrain, ytest=train_test_split(x, y, random_state=12,\n",
    "             test_size=0.15)\n",
    "# with new parameters\n",
    "gbr = GradientBoostingRegressor(n_estimators=600,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.01,\n",
    "    min_samples_split=3)\n",
    "# with default parameters\n",
    "#gbr = GradientBoostingRegressor()\n",
    "\n",
    "gbr.fit(xtrain, ytrain)\n",
    "\n",
    "ypred = gbr.predict(xtest)\n",
    "\n",
    "\n",
    "x_ax = range(len(ytest))\n",
    "plt.scatter(x_ax, ytest, s=5, color=\"blue\", label=\"original\")\n",
    "plt.plot(x_ax, ypred, lw=0.8, color=\"red\", label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 661
    },
    "id": "FsUNa8Z5g5kI",
    "outputId": "3b5cf381-9368-48d9-aa2d-85cdb46e67bf"
   },
   "outputs": [],
   "source": [
    "#Gradient Boosting Predictions vs Actual\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(ytest, ypred, color='red', alpha=0.5, label='Predictions')\n",
    "plt.plot([min(ytest), max(ytest)], [min(ytest), max(ytest)], color='blue', label='y=x line')\n",
    "plt.title('Gradient Boost: Predictions vs Actual')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "plt.axis('square')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = abs(ypred - ytest)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
    "print(ypred.shape[0])\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / ytest)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(ytest,ypred)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "score = r2_score(ytest, ypred)\n",
    "print(\"The accuracy of our model is {}%\".format(round(score, 2) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "x = data.drop(['Utilization Rate All', 'Year'], axis=1)\n",
    "y = data['Utilization Rate All']\n",
    "\n",
    "xtrain, xtest, ytrain, ytest=train_test_split(x, y, random_state=0,\n",
    "             test_size=0.2)\n",
    "# with new parameters\n",
    "hgbr = HistGradientBoostingRegressor(n_estimators=300,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.01,\n",
    "    min_samples_split=3)\n",
    "# with default parameters\n",
    "#gbr = GradientBoostingRegressor()\n",
    "\n",
    "hgbr.fit(xtrain, ytrain)\n",
    "\n",
    "ypred = hgbr.predict(xtest)\n",
    "\n",
    "\n",
    "x_ax = range(len(ytest))\n",
    "plt.scatter(x_ax, ytest, s=5, color=\"blue\", label=\"original\")\n",
    "plt.plot(x_ax, ypred, lw=0.8, color=\"red\", label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting Predictions vs Actual\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(ytest, ypred, color='red', alpha=0.5, label='Predictions')\n",
    "plt.plot([min(ytest), max(ytest)], [min(ytest), max(ytest)], color='blue', label='y=x line')\n",
    "plt.title('HistGradient Boost: Predictions vs Actual')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "plt.axis('square')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = abs(ypred - ytest)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
    "print(ypred.shape[0])\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / ytest)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(ytest,ypred)\n",
    "print(mse)\n",
    "from sklearn.metrics import r2_score\n",
    "score = r2_score(ytest, ypred)\n",
    "print(\"The accuracy of our model is {}%\".format(round(score, 2) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>param_max_leaf_nodes</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>mean_test_error</th>\n",
       "      <th>std_test_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05929</td>\n",
       "      <td>0.022667</td>\n",
       "      <td>0.003818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.160519</td>\n",
       "      <td>0.023332</td>\n",
       "      <td>0.004235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>300</td>\n",
       "      <td>20</td>\n",
       "      <td>0.047293</td>\n",
       "      <td>0.023431</td>\n",
       "      <td>0.003973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>0.067503</td>\n",
       "      <td>0.023473</td>\n",
       "      <td>0.004274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01864</td>\n",
       "      <td>0.023707</td>\n",
       "      <td>0.004139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>0.083745</td>\n",
       "      <td>0.023943</td>\n",
       "      <td>0.004213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>0.176656</td>\n",
       "      <td>0.024065</td>\n",
       "      <td>0.003806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.215543</td>\n",
       "      <td>0.024229</td>\n",
       "      <td>0.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.125207</td>\n",
       "      <td>0.024447</td>\n",
       "      <td>0.002631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.054511</td>\n",
       "      <td>0.024464</td>\n",
       "      <td>0.003297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>0.297739</td>\n",
       "      <td>0.024933</td>\n",
       "      <td>0.003790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.248463</td>\n",
       "      <td>0.025441</td>\n",
       "      <td>0.004580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.906226</td>\n",
       "      <td>0.033655</td>\n",
       "      <td>0.005785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.061034</td>\n",
       "      <td>0.035393</td>\n",
       "      <td>0.003302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.079415</td>\n",
       "      <td>0.041074</td>\n",
       "      <td>0.003206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.043320</td>\n",
       "      <td>0.003681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.019923</td>\n",
       "      <td>0.043826</td>\n",
       "      <td>0.003801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.019351</td>\n",
       "      <td>0.043880</td>\n",
       "      <td>0.003785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.01724</td>\n",
       "      <td>0.044142</td>\n",
       "      <td>0.003756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.039361</td>\n",
       "      <td>0.044375</td>\n",
       "      <td>0.003969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_max_iter param_max_leaf_nodes param_learning_rate  mean_test_error  \\\n",
       "16            300                    5             0.05929         0.022667   \n",
       "1             100                    5            0.160519         0.023332   \n",
       "6             300                   20            0.047293         0.023431   \n",
       "12            100                   20            0.067503         0.023473   \n",
       "14            300                  100             0.01864         0.023707   \n",
       "9             100                   20            0.083745         0.023943   \n",
       "2              30                   50            0.176656         0.024065   \n",
       "19            100                   10            0.215543         0.024229   \n",
       "0            1000                    2            0.125207         0.024447   \n",
       "7            1000                    2            0.054511         0.024464   \n",
       "13            300                   10            0.297739         0.024933   \n",
       "18             10                    5            0.248463         0.025441   \n",
       "8               3                    5            0.906226         0.033655   \n",
       "5              10                  100            0.061034         0.035393   \n",
       "17              3                    5            0.079415         0.041074   \n",
       "4              10                    2              0.0351         0.043320   \n",
       "15              3                   50            0.019923         0.043826   \n",
       "11              3                   10            0.019351         0.043880   \n",
       "10              3                    5             0.01724         0.044142   \n",
       "3               3                    2            0.039361         0.044375   \n",
       "\n",
       "    std_test_error  \n",
       "16        0.003818  \n",
       "1         0.004235  \n",
       "6         0.003973  \n",
       "12        0.004274  \n",
       "14        0.004139  \n",
       "9         0.004213  \n",
       "2         0.003806  \n",
       "19        0.003700  \n",
       "0         0.002631  \n",
       "7         0.003297  \n",
       "13        0.003790  \n",
       "18        0.004580  \n",
       "8         0.005785  \n",
       "5         0.003302  \n",
       "17        0.003206  \n",
       "4         0.003681  \n",
       "15        0.003801  \n",
       "11        0.003785  \n",
       "10        0.003756  \n",
       "3         0.003969  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import loguniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "x = data.drop(['Utilization Rate All', 'Year'], axis=1)\n",
    "y = data['Utilization Rate All']\n",
    "xtrain, xtest, ytrain, ytest=train_test_split(x, y, random_state=0,\n",
    "             test_size=0.2)\n",
    "\n",
    "param_distributions = {\n",
    "    \"max_iter\": [3, 10, 30, 100, 300, 1000],\n",
    "    \"max_leaf_nodes\": [2, 5, 10, 20, 50, 100],\n",
    "    \"learning_rate\": loguniform(0.01, 1),\n",
    "}\n",
    "search_cv = RandomizedSearchCV(\n",
    "    HistGradientBoostingRegressor(),\n",
    "    param_distributions=param_distributions,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_iter=20,\n",
    "    random_state=0,\n",
    "    n_jobs=2,\n",
    ")\n",
    "search_cv.fit(xtrain, ytrain)\n",
    "\n",
    "columns = [f\"param_{name}\" for name in param_distributions.keys()]\n",
    "columns += [\"mean_test_error\", \"std_test_error\"]\n",
    "cv_results = pd.DataFrame(search_cv.cv_results_)\n",
    "cv_results[\"mean_test_error\"] = -cv_results[\"mean_test_score\"]\n",
    "cv_results[\"std_test_error\"] = cv_results[\"std_test_score\"]\n",
    "cv_results[columns].sort_values(by=\"mean_test_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On average, our HGBT regressor makes an error of 0.03 k$\n"
     ]
    }
   ],
   "source": [
    "error = -search_cv.score(xtest, ytest)\n",
    "print(f\"On average, our HGBT regressor makes an error of {error:.2f} k$\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
